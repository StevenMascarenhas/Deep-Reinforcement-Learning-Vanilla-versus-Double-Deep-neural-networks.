# Deep-Reinforcement-Learning-Vanilla-versus-Double-Deep-neural-networks.
To implement a Reinforcement Learning (RL) agent using a Deep Q Network (DQN) applied to the game of Atari in the OpenAI Gym environment
Deepâ€¯Reinforcement Learning: Teaching an Atari Breakout Agent with DQN & Doubleâ€¯DQN



ğŸ“‘ Table of Contents

Why Reinforcement Learning?

The Environment & Setup

DQN Implementations

Training & Evaluation

Added Value â€“ PER & Tuning

Extending This Work

Key References

License

1. Why Reinforcement Learning?

Deep Reinforcement Learning (DRL) lets machines learn behaviour directly from highâ€‘dimensional sensory input by maximising longâ€‘term reward.Â Breakout is an excellent testbed: sparse reward, rich visuals, delayed credit assignment and a nonâ€‘trivial optimal strategy (precise paddle control, tunnel carving, ball trapping).

This repo compares Vanilla DQN (Mnihâ€¯etâ€¯al.,â€¯2013/15) and Double DQN (vanâ€¯Hasseltâ€¯etâ€¯al.,â€¯2016) to demonstrate how overâ€‘estimation bias in Qâ€‘learning harms performance and how a conceptually simple fix yields faster, more stable learning.

Assignment ObjectiveÂ Â (CS6482 Deep RL â€¢Â SemesterÂ 1 AYâ€¯2024/25)Train both agents for 3â€¯000 episodes and evaluate them with the nearâ€‘greedy policy (Îµâ€¯=â€¯0.05) prescribed by Mnihâ€¯etâ€¯al. Report mean score, learning speed and stability, and discuss design tradeâ€‘offs.

2. The Environment & Setup

2â€¯a. Environment: BreakoutNoFrameskipâ€‘v4

Property

Value

API

OpenAI Gym ALE interface

Observation

Raw 210â€¯Ã—â€¯160â€¯Ã—â€¯3 RGB frame

Preâ€‘processing

 crop play area â€¢ downâ€‘sample to 84â€¯Ã—â€¯84 â€¢ convert to grayscale â€¢ normalise to [0,1]

Frame Stack

4 consecutive frames (temporal context)

Action Space

4 discrete: NOOP, FIRE, RIGHT, LEFT

Reward Clipping

+1 for breaking a brick, ball lost = âˆ’1

Episode Terminates

LifeÂ =Â 0 or scoreÂ âˆ¼Â 800

Wrapper StackÂ Â MaxAndSkipEnv(4) â†’ EpisodicLifeEnv â†’ FireResetEnv â†’ WarpFrame(84) â†’ FrameStack(4).

2â€¯b. Dependencies

PythonÂ â‰¥â€¯3.10

PyTorchÂ 2.2Â (CUDAâ€¯12)

GymnasiumÂ 0.29 (Atari extras)

NumPy, tqdm, tensorboard, wandb (optional)

pip install -r requirements.txt
# plus ROMs
pip install autorom[accept-rom-license] && AutoROM

2â€¯c. QuickÂ Start

#Â Clone & install
git clone https://github.com/<yourâ€‘handle>/dqn-breakout.git
cd dqn-breakout
pip install -r requirements.txt

#Â Run the notebook (endâ€‘toâ€‘end)
jupyter notebook Deep\ Reinforcement_Learning_DQN_Double_DQN.ipynb

Full training (3â€¯000â€¯episodes) â‰ˆâ€¯3â€¯h on an RTXâ€¯3060; â‰ˆâ€¯8â€¯h on a 4â€‘core CPU.

2â€¯d. Repository Structure

â”œâ”€â”€ Deep Reinforcement_Learning_DQN_Double_DQN.ipynb   â† Main notebook
â”œâ”€â”€ src/                                               â† Agent classes & utils
â”‚   â”œâ”€â”€ dqn_agent.py             â† VanillaÂ DQN class
â”‚   â”œâ”€â”€ double_dqn_agent.py      â† DoubleÂ DQN subclass
â”‚   â”œâ”€â”€ replay_buffer.py         â† Uniform & Prioritised buffer
â”‚   â””â”€â”€ wrappers.py              â† Atari preâ€‘processing wrappers
â”œâ”€â”€ train.py                                           â† Headless CLI entryâ€‘point
â”œâ”€â”€ models/                                            â† Saved .pth checkpoints
â”œâ”€â”€ results/                                           â† CSV metrics, gifs, plots
â”œâ”€â”€ docs/                                              â† Images for README
â””â”€â”€ requirements.txt

3. DQN Implementations

3â€¯a. Network Architecture

Input: 4 Ã— 84 Ã— 84
 â”œâ”€ Conv2d 32@8Ã—8 stride 4 â”€ ReLU
 â”œâ”€ Conv2d 64@4Ã—4 stride 2 â”€ ReLU
 â”œâ”€ Conv2d 64@3Ã—3 stride 1 â”€ ReLU
 â†’ Flatten (3136)
 â”œâ”€ Linear 512               â”€ ReLU
 â””â”€ Linear |A| (=4)          â†’ Qâ€‘values

â‰ˆâ€¯1.7â€¯M parameters, identical for both agents.

3â€¯b. VanillaÂ DQN

Standard single Qâ€‘network; TD target uses max over nextâ€‘state Qâ€‘values from the same network, leading to upwardâ€‘biased estimates.

3â€¯c. DoubleÂ DQN

Nextâ€‘action selection via online network, but evaluation via frozen target network:TD_target = r + Î³ Â· Q_target(sâ€², argmax_aâ€² Q_online(sâ€²,â€¯aâ€²))Â 
reducing overâ€‘optimistic updates.

3â€¯d. Hyperâ€‘parameters

Hyperâ€‘parameter

Value

Rationale

Replay buffer size

1â€¯000â€¯000

Matches DeepMind setting

Min replay size before learning

50â€¯000

Ensures diverse minibatch

Batch size

32

Discount Î³

0.99

Longâ€‘horizon credit

Learning rate

1â€¯eâ€‘4 (Adam)

Tuned w/ PER sweep

Target net update

every 10â€¯000 envâ€‘steps

Stable targets

Gradient clip

Â±10

Prevent exploding grad

Îµâ€‘decay schedule

1.0 â†’ 0.05 over 1â€¯M steps

Encourages exploration

Prioritised replay Î±

0.6

Schaulâ€¯etâ€¯al. default

Importance sampling Î²

0.4 â†’ 1.0 linearly

Bias correction

4. Training & Evaluation

4â€¯a. Training Pipeline

Initialise environment & agent, fill replay buffer with random policy.

For each step:

Select action via Îµâ€‘greedy (policy net).

Execute, store transition (s,a,r,sâ€²,done).

Every 4 envâ€‘steps learn:

Sample minibatch.

Compute TDâ€‘loss (Huber).

Backprop & clip grads.

Every 10â€¯000 envâ€‘steps sync target net.

Every 100â€¯episodes save checkpoint & log TensorBoard.

4â€¯b. Evaluation Protocol

Nearâ€‘greedy: Îµâ€¯=â€¯0.05 fixed.

20 evaluation episodes, no learning, lifeâ€‘loss NOT treated as terminal (to match Atari benchmarks).

Report mean & stdâ€¯Â±â€¯95â€¯% CI of episode score.

python src/eval.py --checkpoint models/double_dqn_ep3000.pth \
                   --env BreakoutNoFrameskip-v4 --episodes 20

4â€¯c. Results

Metric (3â€¯000â€¯episodes)

VanillaÂ DQN

Doubleâ€¯DQN

Mean bricks/game

â€¯12.7â€¯Â±â€¯8.1

20.1â€¯Â±â€¯10.0

Median

â€¯10

18

Best episode score

â€¯69

96

Learning begins

â€¯35â€¯k steps

28â€¯k

Wallâ€‘clock to 15â€¯avg reward

â€¯2â€¯hâ€¯12â€¯m

1â€¯hâ€¯05â€¯m

DoubleÂ DQN reaches the assignmentâ€™s â‰¥â€¯17â€¯brick benchmark 1.9Ã— faster and finishes 58â€¯% higher than vanilla.

4â€¯d. Watching the Agent

python src/record_gif.py --checkpoint models/double_dqn_ep3000.pth \
                        --env BreakoutNoFrameskip-v4 --output docs/demo.gif

Creates a 60â€¯fps gif in docs/ (23â€¯MB).

5. Added Value â€“Â PER & Tuning

Prioritised Experience Replay (PER), Î±â€¯=â€¯0.6, Î²â€‘annealing; improves sample efficiency ~â€¯25â€¯% (see notebook sec.â€¯8.2).

Learning rate & Îµâ€‘schedule sweeps (WeightsÂ &Â Biases sweeps) â€” best config already baked into defaults.

Gradient clipping & WeightNorm reduce catastrophic Qâ€‘explosion incidents (vanillaÂ DQN suffered 4 collapses vs 0 with clipping).

Mixed Precision (AMP) via torch.cuda.amp yields ~â€¯35â€¯% speedâ€‘up on Ampere GPUs.

6. Extending This Work

Idea

Howâ€‘to

Rainbow Agent

Integrate nâ€‘step returns, NoisyNets, C51 â€“ src/dqn_agent.py uses mixâ€‘in pattern; dropâ€‘in modules welcome.

JAX / Equinox

Replace network + optimiser, keep buffer & wrappers; expect 1.3â€‘1.5â€¯Ã— faster.

MuZeroâ€‘style planning

Swap Qâ€‘network with dynamics + value + policy heads (research project).

Different games

Pass --env PongNoFrameskip-v4 or any Gym Atari id; network autoâ€‘adjusts output dim.

Curriculum learning

Start with slower frameâ€‘skip or larger paddle, gradually raise difficulty.

PRs & detailed issues are â™¥ welcomed.

7. Key References

MnihÂ etâ€¯al., NIPSÂ Deep Learning Workshop, 2013 â€“ "Playing Atari with Deep RL".

vanâ€¯HasseltÂ etâ€¯al., AAAI, 2016 â€“ "Double Qâ€‘learning".

SchaulÂ etâ€¯al., ICLR, 2016 â€“ "Prioritized Experience Replay".

HesselÂ etâ€¯al., AAAI, 2018 â€“ "Rainbow: Combining Improvements in Deep RL".

CastroÂ etâ€¯al., IJCAI, 2018 â€“ "Dopamine" (clean RL codeâ€‘base inspiration).

Comprehensive bibliography with bibtex available in the notebookâ€™s final cell.
